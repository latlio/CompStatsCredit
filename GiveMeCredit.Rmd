---
title: "Comp Stats | Give Me Some Credit"
author: "Lathan Liou, Shivang Mehta, Isaac Cui, Abby Lewis"
date: "11/21/2018"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

#### Libraries ####
library(readxl)
library(tidyr)
library(VIM)
library(readr)
library(dplyr)
library(skimr)
library(glmnet)
cs_training <- read_csv("~/Desktop/Senior Year/CompStats/Chandler/DataProject/cs-training.csv")

#### Data Preparation ####
#remove first column
cs_training <- cs_training[,-1]

#making all columns numeric
cs_training <- sapply(cs_training, as.numeric)

cs_training <- as.data.frame(cs_training)
```

## Section 1: Data Imputation 

```{r}
#EDA
md.pattern(cs_training)
aggr_plot <- aggr(cs_training, col=c('dodgerblue','black'), numbers=TRUE, sortVars=TRUE, labels=names(cs_training), cex.axis=.4, gap=0.5, ylab=c("Histogram of missing data","Pattern"))
```

Write some observations.

# Imputing NumberOfDependents as median

```{r}
median.numdep <- median(cs_training$NumberOfDependents, na.rm = TRUE)
for(i in 1:length(cs_training$NumberOfDependents)){
  if(is.na(cs_training$NumberOfDependents[i])){
    cs_training$NumberOfDependents[i] = median.numdep
  }
}
```

# Imputing MonthlyIncome

```{r}
train1 <- cs_training
train1 <- train1 %>% drop_na()
train.y <- train1$MonthlyIncome
train1 <- subset(train1, select = -c(MonthlyIncome, SeriousDlqin2yrs))
```

# Regression

```{r}
fit <- lm(train.y ~., data=train1)
pred.miss <- fit$coefficients[1] + fit$coefficients[2]*cs_training$RevolvingUtilizationOfUnsecuredLines + fit$coefficients[3]*cs_training$age +
              fit$coefficients[4]*cs_training$`NumberOfTime30-59DaysPastDueNotWorse` + fit$coefficients[5]*cs_training$DebtRatio +
              fit$coefficients[6]*cs_training$NumberOfOpenCreditLinesAndLoans + fit$coefficients[7]*cs_training$NumberOfTimes90DaysLate +
              fit$coefficients[8]*cs_training$NumberOfOpenCreditLinesAndLoans + fit$coefficients[9]*cs_training$`NumberOfTime60-89DaysPastDueNotWorse` +
              fit$coefficients[10]*cs_training$NumberOfDependents

for(i in 1:length(cs_training$MonthlyIncome)){
  if(is.na(cs_training$MonthlyIncome[i])){
    cs_training$MonthlyIncome[i] = pred.miss[i]
  }
}
```

Maybe write a brief description of what the code did.

## Section 2: Feature Engineering

In class, we learned how adding basis functions could improve the way we understand our data: namely finding our decision rules in higher dimensions. Thus, feature engineering is a crucial part of the machine learning process. By transforming our data into features that we think better represent the underlying problem for our machine learning models, we hope to improve predictive performance. We tried to use our domain knowledge about credit scores and our creativity to come up with new features. We ultimately came up with 14 new features in addition to the existing 10 explanatory ones in the data set. 

```{r}
#### remove data that doesn't make sense ####

# age of 0 doesn't make sense
cs_training <- cs_training %>%
  filter(age != 0)

# negative monthly income? maybe that's a typo 
cs_training$MonthlyIncome <- abs(cs_training$MonthlyIncome)

#add features
cs_training <- cs_training %>%
  mutate(Distributedincome = MonthlyIncome/(1+NumberOfDependents)-1,
         Totalcost = DebtRatio * MonthlyIncome,
         Retired = ifelse(age > 65, 1, 0),
         Lateindex = `NumberOfTime30-59DaysPastDueNotWorse` + 2*`NumberOfTime60-89DaysPastDueNotWorse` + 3*NumberOfTimes90DaysLate,
         Numedepencubed = NumberOfDependents^3,
         LogRUUL = ifelse(RevolvingUtilizationOfUnsecuredLines == 0, 0, log(RevolvingUtilizationOfUnsecuredLines)),
         LogMI = ifelse(MonthlyIncome == 0, 0, log(MonthlyIncome)),
         LoansAge = (NumberOfOpenCreditLinesAndLoans + NumberRealEstateLoansOrLines)/age,
         DepAge = NumberOfDependents/age,
         LoansIncome = NumberOfOpenCreditLinesAndLoans + NumberRealEstateLoansOrLines
)
```

```{r}
#save dataset
write_csv(cs_training, "cs_training_final.csv")
```

## Section 3: Feature Selection

```{r}
xmat <- as.matrix(cs_training[,-1])
lassovar <- glmnet(xmat, y=as.factor(cs_training$SeriousDlqin2yrs), alpha=1, family="binomial")
```